{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3 - Evaluating Contrastive CLIP Model on Out of Domain Data to Study Covariate Shift - PACS Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making All the necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the CLIP Model and Processor. The Model that we will be using is \"openai/clip-vit-base-patch32\". We will also have processor which processes the image and text data required by CLIP-ViT-Base. We also move the model to GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akhan\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Function below is used to load in the PACS Dataset. It essentially travels through all subfolders of all the domains and returns for each of them an np.array of image and text pairs which we can then further use for zero shot image classification and other purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_pacs_dataset(dataset_path):\n",
    "    domains = ['art_painting', 'cartoon', 'photo', 'sketch']\n",
    "    classes = ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    for domain in domains:\n",
    "        for class_idx, class_name in enumerate(classes):\n",
    "            class_dir = os.path.join(dataset_path, domain, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_array = np.array(img)\n",
    "                images.append(img_array)\n",
    "                labels.append(class_idx)\n",
    "    \n",
    "    return np.array(images), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Call to load the dataset In."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 9991 images and 9991 labels from the PACS dataset.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = './PACS'\n",
    "images, labels = load_pacs_dataset(dataset_path)\n",
    "print(f\"Loaded {len(images)} images and {len(labels)} labels from the PACS dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the class names present in the PACS Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['dog', 'elephant', 'giraffe', 'guitar', 'horse', 'house', 'person']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where Cosine Similarity and predictions are made. Image and text embeddings are made, normalized and there cosine similarity computed. In the end predict the labels based on the highest cosine similarity. This is done for the complete dataset and accuracy is reported at the end. This should be Slightly Lower than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification(images, labels):\n",
    "    text_inputs = processor(text=classes, return_tensors=\"pt\", padding=True).to(device)\n",
    "    text_features = model.get_text_features(**text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for img, label in zip(images, labels):\n",
    "\n",
    "        \n",
    "        image_input = processor(images=img, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(**image_input)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        similarity = (image_features @ text_features.T).squeeze(0)\n",
    "        \n",
    "        pred_class = similarity.argmax().item()\n",
    "        \n",
    "        predictions.append(pred_class)\n",
    "        true_labels.append(label)\n",
    "    \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving the function call for zero_shot_classification class to get accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot classification accuracy on PACS: 81.34%\n"
     ]
    }
   ],
   "source": [
    "accuracy = zero_shot_classification(images, labels)\n",
    "print(f\"Zero-shot classification accuracy on PACS: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
