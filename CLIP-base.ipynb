{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:02<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot classification accuracy: 52.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os \n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "class StyledImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # List all image files starting with 'styled_image_'\n",
    "        self.image_filenames = [f for f in os.listdir(root_dir) if f.startswith('styled_image_')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.image_filenames[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        label = int(self.image_filenames[idx].split('_')[2].split('.')[0])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Define the directory containing your dataset\n",
    "data_dir = './CIFAR_Styled'\n",
    "\n",
    "# Mapping folder names to class indices (CIFAR-10 class names)\n",
    "class_map = {\n",
    "    'airplane': 0,\n",
    "    'automobile': 1,\n",
    "    'bird': 2,\n",
    "    'cat': 3,\n",
    "    'deer': 4,\n",
    "    'dog': 5,\n",
    "    'frog': 6,\n",
    "    'horse': 7,\n",
    "    'ship': 8,\n",
    "    'truck': 9\n",
    "}\n",
    "\n",
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to fit the model input size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = StyledImageDataset(root_dir=data_dir, transform=transform)\n",
    "test_loader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Prepare text features for the classes\n",
    "class_names = list(class_map.keys())\n",
    "text_inputs = processor(text=class_names, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Zero-shot classification function\n",
    "def zero_shot_classification(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            # Get image features\n",
    "            image_features = model.get_image_features(images)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)  # Normalize image features\n",
    "\n",
    "            # Get text features\n",
    "            text_features = model.get_text_features(**text_inputs)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)  # Normalize text features\n",
    "\n",
    "            # Compute similarity scores\n",
    "            logits_per_image = image_features @ text_features.T  # Dot product\n",
    "            probs = logits_per_image.softmax(dim=-1)  # Convert to probabilities\n",
    "\n",
    "            # Predicted class indices\n",
    "            predictions = probs.argmax(dim=-1)\n",
    "\n",
    "            # Update correct predictions count\n",
    "            correct += (predictions.cpu() == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = zero_shot_classification(test_loader)\n",
    "print(f'Zero-shot classification accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
