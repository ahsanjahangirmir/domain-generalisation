{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Shape Bias Of Clip-ViT-Base "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making All the necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Names in the CIFAR-10G Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR10G_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Dataset Class To Load In CIFAR-10G Dataset. There are 6 subdirectories within the main folder which hold Stylised out-of-domain generalisation test images. We loop through each Subdirectory and within each subdirectory are 10 folder with CIFAR-10 Class name folders. The following Class Loops through all categories within the folders and gets all images in the category directory. IThe Get method applies transformations to the image and returns the image with transformations applied to it along with the label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCIFAR10(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        for image_type in ['line_drawings', 'line_drawings_inverted', 'contours', \n",
    "                           'contours_inverted', 'silhouettes', 'silhouettes_inverted']:\n",
    "            image_type_dir = os.path.join(root_dir, image_type)\n",
    "            \n",
    "            for idx, category in enumerate(CIFAR10G_CLASSES):\n",
    "                category_dir = os.path.join(image_type_dir, category)\n",
    "                \n",
    "                for img_file in os.listdir(category_dir):\n",
    "                    if img_file.endswith(('.png', '.jpg', '.jpeg')): \n",
    "                        img_path = os.path.join(category_dir, img_file)\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(idx) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            \n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define and make custom transformations So that CLIP processor can 1 work with a torch vision dataset and 2 enhance the dataset and help the model in Image classification. transforms.Grayscale(num_output_channels=1) this was done so that we set output channels to one and we define image is in greyscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define the route to the CIFAR-10G Dataset along with that load in the Dataset. We also here define the dataloader and split the cifar-10G dataset into chunks of size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = './CIFAR-10G-Ahsan'\n",
    "dataset = CustomCIFAR10(root_dir=root_dir, transform=transform)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the CLIP Model and Processor. The Model that we will be using is \"openai/clip-vit-base-patch32\". We will also have processor which processes the image and text data required by CLIP-ViT-Base. We also move the model to GPU if available. The central modification was done as the image had to be converted to a Greyscale image of dimension of one and we had to make clip work in with one dimension images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "with torch.no_grad():\n",
    "    model.vision_model.embeddings.patch_embedding.weight = torch.nn.Parameter(\n",
    "        model.vision_model.embeddings.patch_embedding.weight.mean(dim=1, keepdim=True)\n",
    "    )\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function For zero shot image classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where Cosine Similarity and predictions are made. Image and text embeddings are made, normalized and there cosine similarity computed. In the end predict the labels based on the highest cosine similarity. This is done for the complete dataset and accuracy is reported at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_shot_classification(model, processor, dataloader, device):\n",
    "    text_inputs = processor(text=CIFAR10G_CLASSES, return_tensors=\"pt\", padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_images = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for images, labels in tqdm(dataloader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.get_image_features(images)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)  \n",
    "\n",
    "        predicted_class = similarity.argmax(dim=1)\n",
    "\n",
    "        correct_predictions += (predicted_class == labels).sum().item()\n",
    "        total_images += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_images\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy We got On CIFAR-10 Was 87.06 hence that will be our total accuracy. To calculate our shape Bias we will use the following formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Shape Bias} = \\frac{\\text{Shape Accuracy}}{\\text{Total Accuracy}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Output we will get will go in the numerator. So in the following cell we will do the function call and the resultant math to get Shape Bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:38<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape Bias is : 77.724%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = zero_shot_classification(model, processor, dataloader, device)\n",
    "\n",
    "print(f\"Shape Bias is : {(accuracy*100/87.06)*100:.3f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
